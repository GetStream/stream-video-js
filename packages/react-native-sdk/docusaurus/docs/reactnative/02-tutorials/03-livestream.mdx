---
title: Livestream Tutorial
description: How to build a livestream experience using Stream's React Native Video SDK
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

import { TokenSnippet } from '../../../shared/_tokenSnippet.jsx';

In this tutorial, we'll quickly build a low-latency in-app livestreaming experience.
The livestream is broadcasted using Stream's edge network of servers around the world.

We'll cover the following topics:

- Ultra low latency streaming
- Multiple streams & co-hosts
- RTMP in and WebRTC input
- Exporting to HLS
- Reactions, custom events and chat
- Recording & Transcriptions

Let's get started, if you have any questions or feedback be sure to let us know via the feedback button.

## Step 1 - Setup a new React Native app

Create a new React Native app using the official template,

```bash title=Terminal
npx react-native@latest init VideoCallExample
cd VideoCallExample
```

## Step 2 - Install the SDK and declare permissions

In order to install the Stream Video React Native SDK, run the following command in your terminal of choice:

```bash title=Terminal
yarn add @stream-io/video-react-native-sdk @stream-io/react-native-webrtc
```

The SDK requires installing some peer dependencies. You can run the following command to install them:

```bash title=Terminal
yarn add react-native-incall-manager@4.1.0
yarn add react-native-svg
yarn add @react-native-community/netinfo@9.3.9
yarn add @notifee/react-native@7.7.1

# Install pods for iOS
npx pod-install
```

#### Add Stream Video SDK's setup method

<Tabs groupId="device-os">
<TabItem value="android" label="Android" default>

Add the following in your `MainApplication.java` file:

<!-- vale off -->

```java
// highlight-next-line
import com.streamvideo.reactnative.StreamVideoReactNative;

public class MainApplication extends Application implements ReactApplication {

  @Override
  public void onCreate() {
    super.onCreate();
    // highlight-next-line
    StreamVideoReactNative.setup();
    // the rest..
  }
}
```

</TabItem>
<TabItem value="ios" label="iOS">

Add the following in your `AppDelegate.m` or `AppDelegate.mm` file:

```c
// highlight-next-line
#import "StreamVideoReactNative.h"

@implementation AppDelegate

- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions
{
  // highlight-next-line
  [StreamVideoReactNative setup];

  // the rest..
}
```

</TabItem>
</Tabs>

#### Declare permissions

<Tabs groupId="device-os">
<TabItem value="android" label="Android" default>

In `AndroidManifest.xml` add the following permissions before the `application` section.

```xml
<manifest xmlns:android="http://schemas.android.com/apk/res/android">
  // highlight-start
  <uses-feature android:name="android.hardware.camera" />
  <uses-feature android:name="android.hardware.camera.autofocus" />
  <uses-feature android:name="android.hardware.audio.output" />
  <uses-feature android:name="android.hardware.microphone" />

  <uses-permission android:name="android.permission.CAMERA" />
  <uses-permission android:name="android.permission.RECORD_AUDIO" />
  <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE" />
  <uses-permission android:name="android.permission.CHANGE_NETWORK_STATE" />
  <uses-permission android:name="android.permission.MODIFY_AUDIO_SETTINGS" />
  // highlight-end
  <uses-permission android:name="android.permission.INTERNET" />
  ...
  <application
    ...
  />
  </application>
</manifest>
```

If you plan to also support Bluetooth devices then also add the following.

```xml
<uses-permission android:name="android.permission.BLUETOOTH" android:maxSdkVersion="30" />
<uses-permission android:name="android.permission.BLUETOOTH_ADMIN" android:maxSdkVersion="30" />
<uses-permission android:name="android.permission.BLUETOOTH_CONNECT" />
```

</TabItem>
<TabItem value="ios" label="iOS">

Add the following keys and values to `Info.plist` file, under `dict` tag.

```plist title="Info.plist"
<plist version="1.0">
<dict>
  ...
  <key>CFBundleName</key>
  <string>$(PRODUCT_NAME)</string>
  // highlight-start
  <key>NSCameraUsageDescription</key>
  <string>$(PRODUCT_NAME) would like to use your camera</string>
  <key>NSMicrophoneUsageDescription</key>
  <string>$(PRODUCT_NAME) would like to use your microphone</string>
  // highlight-end
  ...
</dict>
</plist>

```

</TabItem>
</Tabs>

#### Android Specific installation

In `android/app/build.gradle` add the following inside the `android` section:

```java
android {
  ...
  // highlight-start
  compileOptions {
    sourceCompatibility JavaVersion.VERSION_1_8
    targetCompatibility JavaVersion.VERSION_11
  }
  // highlight-end
}
```

In `android/gradle.properties` add the following:

```groovy
android.enableDexingArtifactTransform.desugaring=false
```

<!-- vale on -->

#### Run the app

To ensure the best possible experience, we highly recommend running the app on a physical device.
This is due to the limitations in audio and video device support on emulators.
You can refer to the React Native documentation for guidance on [running the app on a physical device](https://reactnative.dev/docs/running-on-device).

However, if you still prefer to use an emulator, execute the following command:

```bash
# run iOS app
yarn ios

# run Android app
yarn android
```

## Step 3 - Broadcast a livestream from your device

The following code shows how to publish from your device's camera.
Let's open `App.tsx` and replace its contents with the following code:

```tsx title=App.tsx
import React from 'react';
import {
  StreamCall,
  StreamVideo,
  StreamVideoClient,
  User,
} from '@stream-io/video-react-native-sdk';
import { SafeAreaView, Text } from 'react-native';

const apiKey = 'REPLACE_WITH_API_KEY'; // the API key can be found in the "Credentials" section
const token = 'REPLACE_WITH_TOKEN'; // the token can be found in the "Credentials" section
const userId = 'REPLACE_WITH_USER_ID'; // the user id can be found in the "Credentials" section
const callId = 'REPLACE_WITH_CALL_ID'; // the call id can be found in the "Credentials" section

// initialize the user object
const user: User = {
  id: userId,
  name: 'Santhosh',
  image: `https://getstream.io/random_png/?id=${userId}&name=Santhosh`,
};

const myClient = new StreamVideoClient({ apiKey, user, token });
const myCall = myClient.call('livestream', callId);
myCall.join({ create: true });

export default function App() {
  return (
    <StreamVideo client={myClient} language="en">
      <StreamCall call={myCall}>
        <SafeAreaView style={{ flex: 1 }}>
          <LivestreamUI />
        </SafeAreaView>
      </StreamCall>
    </StreamVideo>
  );
}

const LivestreamUI = () => (
  <Text style={{ fontSize: 30, color: 'black' }}>TODO: render video</Text>
);
```

If you run the app now, you'll see that the client fails to connect.
To fix that, we need to provide valid credentials. They shall be set in the following variables:

```ts
const apiKey = 'REPLACE_WITH_API_KEY';
const token = 'REPLACE_WITH_TOKEN';
const userId = 'REPLACE_WITH_USER_ID';
const callId = 'REPLACE_WITH_CALL_ID';
```

Replace them now with the appropriate values from the table below:

<TokenSnippet sampleApp="livestream" displayStyle="credentials" />

When you run the app now you'll see a text message saying: "TODO: render video". Before we get around to rendering the video let's review the code above.

In the first step, we set up the user:

```ts
const user: User = {
  // any string can be used for the id
  id: userId,
  // name and image are used in the UI
  name: 'Santhosh',
  image: `https://getstream.io/random_png/?id=${userId}&name=Santhosh`,
};
```

If you don't have an authenticated user you can also use a guest or anonymous user. For most apps, it's convenient to match your own system of users to grant and remove permissions.

Next, we initialize the client by passing the API Key, user and user token:

```ts
const myClient = new StreamVideoClient({ apiKey, user, token });
```

Your backend typically generates the `user` and `token` on sign-up or login.

The most important step to review is how we create the call. Stream uses the same call object for livestreaming, audio rooms and video calling. Have a look at the code snippet below:

```ts
const myCall = client.call('livestream', callId);
await myCall.join({ create: true });
```

To create the first call object, specify the call type as **livestream** and provide a unique `callId`. The `livestream` call type comes with default settings that are usually suitable for live streams, but you can customize features, permissions, and settings in the dashboard. Additionally, the dashboard allows you to create new call types as required.

Finally, using `call.join({ create: true })` will not only create the call object on our servers but also initiate the real-time transport for audio and video. This allows for seamless and immediate engagement in the live stream.

Note that you can also add members to a call and assign them different roles. For more information, see the [Joining & Creating Calls](../../core/joining-and-creating-calls/) docs.

:::note
You can also add members to a call and assign them different roles.

Also, in production-grade apps, you'd typically store the `call` instance in a state variable and take care of correctly disposing it.
Read more in our [Joining and Creating Calls](../../core/joining-and-creating-calls/) guide.
:::

## Step 4 - Rendering the video

In this step, we're going to build a UI for showing your local video with a button to start the livestream.

In `App.tsx` replace the `LivestreamUI` component with the following code:

```tsx
import {
  useCall,
  useCallStateHooks,
  useIncallManager,
  VideoRenderer,
} from '@stream-io/video-react-native-sdk';
import { Button, Text, StyleSheet } from 'react-native';

const LivestreamUI = () => {
  const call = useCall();

  const { useParticipantCount, useLocalParticipant, useIsCallLive } =
    useCallStateHooks();

  const totalParticipants = useParticipantCount();
  const localParticipant = useLocalParticipant();
  const isCallLive = useIsCallLive();

  // Automatically route audio to speaker devices as relevant for watching videos.
  useIncallManager({ media: 'video', auto: true });

  return (
    <View style={styles.flexed}>
      <Text style={styles.text}>Live: {totalParticipants}</Text>
      <View style={styles.flexed}>
        {localParticipant && (
          <VideoRenderer
            participant={localParticipant}
            trackType="videoTrack"
          />
        )}
      </View>
      <View style={styles.bottomBar}>
        {isCallLive ? (
          <Button onPress={() => call?.stopLive()} title="Stop Livestream" />
        ) : (
          <Button
            onPress={() => {
              call?.goLive();
            }}
            title="Start Livestream"
          />
        )}
      </View>
    </View>
  );
};

const styles = StyleSheet.create({
  flexed: {
    flex: 1,
  },
  text: {
    alignSelf: 'center',
    color: 'white',
    backgroundColor: 'blue',
    padding: 6,
    margin: 4,
  },
  bottomBar: {
    alignSelf: 'center',
    margin: 4,
  },
});
```

Upon running your app, you will be greeted with an interface that looks like this:

![Preview of the UI](../assets/02-tutorials/03-livestream/livestream-first-preview.png)

Alternatively, you can utilize the [`HostLivestream`](../../ui-components/livestream/host-livestream) component from our SDK, which offers a preconfigured user interface. You have the flexibility to apply your own customizations on top of this default UI.

To do that, you can follow this code snippet:

```tsx
import { HostLivestream } from '@stream-io/video-react-native-sdk';

const LivestreamUI = () => {
  return <HostLivestream />;
};
```

Stream uses a technology called SFU cascading to replicate your livestream over different SFUs around the world. This makes it possible to reach a large audience in real-time.

Now let's press **Start Livestream** in your new app and click the **Join Call** button below to watch your livestream in another tab in your browser:

<TokenSnippet sampleApp="livestream" displayStyle="join" />

Let's take a moment to review the code above.

#### State & Participants​

You can see we use a few hooks to get the call state. The ones used in this example are:

```ts
const { useParticipantCount, useLocalParticipant, useIsCallLive } =
  useCallStateHooks();
```

In this example, we use:

- `useParticipantCount`: the number of participants watching the livestream
- `useLocalParticipant`: the local participant object, to get the video stream.
- `useIsCallLive`: a boolean that returns if the call is in backstage mode or not

There are many possibilities and the [call and participant state](../../core/call-and-participant-state/) docs explain this in more detail.

#### Creating a UI to watch a livestream​

The layout is built using standard React Native components.
The `RTCView` component is provided by `@stream-io/react-native-webrtc` library. You can use it for rendering the local and remote video.
The `useIncallManager` hook is used to automatically route audio to speaker devices as relevant for watching videos.

#### Backstage mode

In the example above you might have noticed the `call.goLive()` method and the `isCallLive` value.
The backstage functionality is enabled by default on the `livestream` call type.
It makes it easy to build a flow where you and your co-hosts can setup your camera and equipment before going live.
Only after you execute `call.goLive()` will regular users be allowed to join the livestream.

This is convenient for many livestreaming and audio-room use cases. If you want calls to start immediately when you join them that's also possible.
Simply go to the Stream dashboard, click the livestream call type and disable the backstage mode.

The `call.goLive` method can also automatically start HLS livestreaming, recording or transcribing. To do that, you need to pass the corresponding optional parameter:

```ts
await call.goLive({
  start_hls: true,
  start_recording: true,
  start_transcription: true,
});
```

## Step 5 - (Optional) Publishing RTMP using OBS

The example above showed how to publish your device camera to the livestream.
Almost all livestream software and hardware supports RTMPS.
[OBS](https://obsproject.com/) is one of the most popular livestreaming software packages and we'll use it to explain how to import RTMPS.
So let's see how to publish using RTMPs. Feel free to skip this step if you don't need to use RTMPs.

#### Log the URL & Stream Key

```ts
import { useCallStateHooks } from '@stream-io/video-react-native-sdk';

const { useCallIngress } = useCallStateHooks();

const ingress = useCallIngress();
// highlight-start
const rtmpURL = ingress?.rtmp.address;
const streamKey = token;
// highlight-end

console.log('RTMP url:', rtmpURL, 'Stream key:', streamKey);
```

#### Open OBS and go to settings -> stream

- Select "custom" service
- Server: equal to the `rtmpURL` from the log
- Stream key: equal to the `streamKey` from the log

Press start streaming in OBS. The RTMP stream will now show up in your call just like a regular video participant.

Now that we've learned to publish using WebRTC or RTMP let's talk about watching the livestream.

## Step 6 - Viewing a livestream (WebRTC)

Watching a livestream is even easier than broadcasting.

Compared to the current code in `App.tsx` you:

- Don't render the local video, but instead render the remote videos - `useRemoteParticipants` instead of `useLocalParticipant`
- Typically include some small UI elements like viewer count, a button to mute etc.

Additionally, you can use the [`ViewerLivestream`](../../ui-components/livestream/viewer-livestream) component from our SDK, which offers the stream viewing capabilities along side the flexibility to apply your own customizations on top of this default UI.

To do that, you can follow this code snippet:

```tsx
import { ViewerLivestream } from '@stream-io/video-react-native-sdk';

const LivestreamUI = () => {
  return <ViewerLivestream />;
};
```

## Step 7 - (Optional) Viewing a livestream with HLS

Another way to watch a livestream is using HLS. HLS tends to have a 10 to 20 seconds delay, while the above WebRTC approach is real-time.
The benefit that HLS offers is better buffering under poor network conditions.
So HLS can be a good option when:

- A 10-20 second delay is acceptable
- Your users want to watch the Stream in poor network conditions

One option to start HLS is to set the `start_hls` parameter to `true` in the `call.goLive()` method, as described above.
If you want to explicitly start it, when you are live, you can use the following method:

```ts
import { useCall, useCallStateHooks } from '@stream-io/video-react-native-sdk';

const call = useCall();
await call.startHLS();
// or alternatively:
// await call.goLive({ start_hls: true });

// take the URL to the M3U8 playlist
const { useCallEgress } = useCallStateHooks();
const egress = useCallEgress();
// highlight-next-line
const playlistUrl = egress?.hls?.playlist_url;
console.log('HLS playlist url:', playlistUrl);
```

You can view the HLS video feed using any HLS capable video player. The most popular one on React Native is the one provided by [`react-native-video`](https://github.com/react-native-video/react-native-video) library.

## Step 8 - Advanced Features

This tutorial covered broadcasting and watching a livestream.
It also went into more details about HLS & RTMP-in.

There are several advanced features that can improve the livestreaming experience:

- ** [Co-hosts](../../core/joining-and-creating-calls/) ** You can add members to your livestream with elevated permissions. So you can have co-hosts, moderators etc.
- ** [Custom events](../../core/reactions-and-custom-events/) ** You can use custom events on the call to share any additional data. Think about showing the score for a game, or any other real-time use case.
- ** [Reactions & Chat](../../core/reactions-and-custom-events/) ** Users can react to the livestream, and you can add chat. This makes for a more engaging experience.
- ** [Recording](../../advanced/recording/) ** The call recording functionality allows you to record the call with various options and layouts.
- ** [Notifications](../../advanced/push-notifications/overview/) ** You can notify users via push notifications when the livestream starts.

## Recap

It was fun to see just how quickly you can build in-app low-latency livestreaming.
Please do let us know if you run into any issues.
Our team is also happy to review your UI designs and offer recommendations on how to achieve them with Stream.

To recap what we've learned:

- WebRTC is optimal for latency, HLS is slower but buffers better for users with poor connections
- You setup a call: `const call = client.call("livestream", callId)`
- The call type `"livestream"` controls which features are enabled and how permissions are setup
- The livestream by default enables "backstage" mode. This allows you and your co-hosts to setup your mic and camera before allowing people in
- When you join a call, real-time communication is setup for audio & video: `await call.join()`
- Call state `call.state` and helper state access hooks make it easy to build your own UI

We've used [Stream's Livestream API](https://getstream.io/video/livestreaming/),
which means calls run on a global edge network of video servers.
By being closer to your users the latency and reliability of calls are better.
The React SDK enables you to build in-app [video calling, audio rooms and livestreaming](https://getstream.io/video/) in days.

We hope you've enjoyed this tutorial and please do feel free to reach out if you have any suggestions or questions.
