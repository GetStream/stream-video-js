---
title: Livestream Tutorial
description: How to build a live stream experience using Stream's React Video SDKs
---

## Quickstart

In this quickstart, we'll build a live-streaming experience that is similar to Twitch.

- The live stream will run on Stream's Edge network of servers around the world
- We can use ultra-low latency ([WebRTC](https://webrtc.org/)) based live streaming or [HLS](https://en.wikipedia.org/wiki/HTTP_Live_Streaming) (slight delay, but better buffering)
- We will go through the process of publishing the live stream from the browser
- Stream enables us to automatically scale to millions of viewers (no actions required from your side)
- We even show how to have multiple active participants in the live stream

The app that we are going to build will work in two modes: Host (presenter) and Viewer (audience).

The host will be able to publish the live stream and the viewers will be able to watch the live stream.
If you want to see a full-blown example, check out this [sample app.](https://github.com/GetStream/stream-video-js/tree/main/sample-apps/react/livestream-app)

Let's get started.

## Step 1 - Preparations

Before going into the code, let's make sure we have everything we need.

- Add Stream's React Video SDK to your project:
  - `yarn add @stream-io/video-react-sdk` or
  - `npm install @stream-io/video-react-sdk`
- Setup a Stream account at [GetStream's Dashboard](https://dashboard.getstream.io) or use your existing one
  - We will need an API key and secret to run the project

## Step 2 - Setup the host mode

Hosts will communicate with each other in real-time.
For that purpose, we will use the SDK-provided `<StreamCall /`> component to establish a WebRTC-based call for the hosts.

:::note
In this tutorial, we will skip the details on how a `StreamVideoClient` can be set up. You can read more about it on the following page:
[Authenticating your Users](../../guides/client-auth/)
:::

In the code snippet provided, we establish a connection to the Stream Edge server as the live stream host. We then proceed to join the call and publish our audio and video content to the live stream.
In this example, we use the predefined [`<PaginatedGridLayout>`](../../ui-components/core/call-layout/#paginatedgridlayout) for the live stream.

If you prefer a more customized layout, you can build your own. See our article about [Building Custom Layouts.](../ui-cookbook/custom-call-layout/)

```tsx
import {
  useCall,
  useCallCallingState,
  useCreateStreamVideoClient,
  useMediaDevices,
  CancelCallButton,
  PaginatedGridLayout,
  StreamCall,
  StreamVideo,
  ToggleAudioPublishingButton,
  ToggleCameraPublishingButton,
} from '@stream-io/video-react-sdk';

// ... API key, provider, and user setup

export const HostApp = () => {
  const client = useCreateStreamVideoClient({
    apiKey,
    tokenOrProvider,
    user,
  });

  return (
    <StreamVideo client={client}>
      <StreamCall
        callType="livestream"
        callId="some-random-call-id"
        autoJoin={true}
        data={{
          create: true,
          // optionally, invite other hosts to the live stream
          data: {
            members: [...],
          }
        }}
      >
        <BackstageUI />
      </StreamCall>
    </StreamVideo>
  )
}

const BackstageUI = () => {
  const call = useCall();
  const callState = useCallCallingState();
  // useMediaDevices() hook provides you quick access to the Browser's MediaDevices API
  // and a few helper methods to manage your audio and video streams.
  const { publishVideoStream, publishAudioStream } = useMediaDevices();

  useEffect(() => {
    if (!call) return;
    if (callState === CallingState.JOINED) {
      // once the call is joined, publish the host's audio and video
      publishVideoStream();
      publishAudioStream();
    }
  }, [call, callState, publishAudioStream, publishVideoStream]);

  return (
    <>
      <BackstageHeader />
      // use a predefined layout from the SDK or build your own
      <PaginatedGridLayout />
      <BackstageControls />
    </>
  );
};

const BackstageControls = () => {
  return (
    <div className="backstage-controls">
      <ToggleAudioPublishingButton />
      <ToggleCameraPublishingButton />
      <CancelCallButton />
      // The implementation of this button is shown in the next step
      <ToggleLivestreamButton />
    </div>
  );
};

```

With this done, we have a fully functional livestream experience for the host.
The hosts can join the live stream, publish their audio and video, and invite other hosts to the live stream.

:::note
When the call is started, the live streaming isn't enabled for the viewers yet.
We will see how to enable it in the next step.
:::

## Step 3 - Enable the live stream for the viewers

For simplicity, we are going to implement an additional button in the `BackstageControls` component that will enable the live stream for the viewers.

The button should be able to toggle the live stream on and off. For that purpose, we are going to use two utility hooks from the SDK:

- [`useCall`](../04-call-engine/hooks-and-contexts.md#usecall) and
- [`useIsCallBroadcastingInProgress`](../04-call-engine/hooks-and-contexts.md#useiscallbroadcastinginprogress)

The `useCall` hook provides us with the current call object. This is useful when we want to access the call object from a component that is deep in the component tree with the `<StreamCall />` component as a predecessor.

On the other hand, the `useIsCallBroadcastingInProgress` hook gives us the current broadcasting state of the call. This is helpful when we want to show the current state of the live stream or build custom UI components or flows based on this call state.

The [`call`](../04-call-engine/Call.md) instance itself provides two methods for starting and stopping the live stream:

- [`call.startBroadcasting()`](../04-call-engine/Call.md#startbroadcasting) and
- [`call.stopBroadcasting()`](../04-call-engine/Call.md#stopbroadcasting)

Knowing all of this, let's write the code for the button:

```tsx
const ToggleLivestreamButton = () => {
  const call = useCall();
  const isBroadcasting = useIsCallBroadcastingInProgress();
  return (
    <Button
      onClick={() => {
        if (isBroadcasting) {
          call.stopBroadcasting();
        } else {
          call.startBroadcasting();
        }
      }}
    >
      {isBroadcasting ? 'Stop Stream' : 'Start Stream'}
    </Button>
  );
};
```

:::info

Starting and stopping a live stream for the viewers is an operation that takes a few seconds to complete.
It is a good practice to show a loading indicator while the operation is in progress.

There are two ways how we can do that:

- have a `useEffect` that would run whenever we have initiated a change in the broadcasting state of the call until the value of `isBroadcasting` toggles.
- attach an event listener to the call object and follow the delivery of `call.broadcasting_started` and `call.broadcasting_stopped` events. Read more about events [here.](../../advanced/events)

:::

## Step 4 - Setup the viewer mode

The viewer mode is very similar to the host mode. The only difference is that the viewer doesn't have to publish their audio and video to the live stream.

A viewer can join a live stream in two ways:

- WebRTC-based live streaming (ultra-low latency), by using Stream Edge servers
- HLS-based live streaming (slight delay, but better buffering), by using Stream CDN

### WebRTC-based live streaming

For the WebRTC, we are going to use the same `<StreamCall />` component as in the host mode. The only difference is that we are going to use an anonymous user for the viewer.

:::caution

Anonymous users can't establish a WebSocket connection to our backend, therefore they are unable to receive any events from the call.
This means that the viewer won't know when the call starts or stops broadcasting.

To circumvent this limitation, the Viewer app should be able to be notified by the Host app or some other backend service when the call starts or stops broadcasting.
Alternatively, you can keep polling the backend for the current state of the call. For more details on this, see [Querying Calls.](../02-guides/06-querying-calls.mdx)

:::

```tsx
import {
  useCallCallingState,
  useCreateStreamVideoClient,
  CallingState,
  LoadingIndicator,
  PaginatedGridLayout,
  StreamCall,
  StreamVideo,
} from '@stream-io/video-react-sdk';

// ... API key, provider, and user setup

export const ViewerApp = (props) => {
  const client = useCreateStreamVideoClient({
    apiKey,
    tokenOrProvider,
    user,
    isAnonymous: true,
  });

  // `callId` needs to be provided from some party.
  // it is up to the developer to decide how to do that (prop, query param, etc.)
  const callId = props.callId;
  const [activeCall, setActiveCall] = useState<Call>();
  useEffect(() => {
    if (!callId) return;
    client
      .queryCalls({
        filter_conditions: {
          cid: { $in: [`livestream:${callId}`] },
        },
        sort: [{ field: 'cid', direction: 1 }],
      })
      .then(({ calls }) => {
        const [call] = calls;
        if (call) {
          setActiveCall(call);
        }
      });
  }, [client, callId]);

  return (
    <StreamVideo client={client}>
      <StreamCall call={activeCall} autoJoin={true}>
        <WebRTCLivestreamUI />
      </StreamCall>
    </StreamVideo>
  );
};

const WebRTCLivestreamUI = () => {
  const isJoined = useCallCallingState() === CallingState.JOINED;
  return (
    <>
      <ViewerHeader />
      // use any layout from the SDK or build your own
      <PaginatedGridLayout />
      // show a loading indicator while the call is joining
      {!isJoined && <LoadingIndicator />}
      <ViewerControls />
    </>
  );
};
```

### HLS-based live streaming

For the HLS scenario, we are going to use one very popular library called `hls.js`.
HLS.js is a JavaScript library that implements an HTTP Live Streaming client. It relies on HTML5 video and MediaSource Extensions for playback.
You can read more about HLS.js [here.](https://github.com/video-dev/hls.js/)

For the HLS-based live streaming, we are going to use a few components from the Stream Video SDK for React. However, instead using a pre-built layout,
we are going to set up the HLS.js library and use it to play the live stream.

In the example below, you will get familiar with the [`useCallMetadata`](../04-call-engine/Call.md#usecallmetadata) hook.
This hook returns the metadata of the current call.
In our case, the metadata will contain the HLS playlist URL that we are going to use to play the live stream.

Compared to the WebRTC example, we would just replace the "UI" component with the following:

```tsx
import { ... } from '@stream-io/video-react-sdk';
import HLS from 'hls.js';

export const ViewerApp = () => {
  // pseudo-code for simplicity, see the previous example for the full code
  const client = useCreateStreamVideoClient({...});
  const [activeCall] = client.queryCalls({...});
  return (
    <StreamVideo client={client}>
      <StreamCall
        call={activeCall}
        // we don't want to join this call, as that would switch to WebRTC-based live streaming.
        // we are going to use HLS.js to play the live stream
        autoJoin={false}
      >
        <HLSLivestreamUI />
      </StreamCall>
    </StreamVideo>
  )
}

export const HLSLivestreamUI = () => {
  // The HLS playlist URL is in call's metadata
  // we'll use the `useCallMetadata()` hook to get it
  const { hls_playlist_url } = useCallMetadata();

  const [isPlaying, setIsPlaying] = useState(false);
  const videoRef = useRef<HTMLVideoElement>(null);
  useEffect(() => {
    if (!call || !hls_playlist_url) return;

    const hls = new HLS();
    hls.loadSource(hls_playlist_url);
    hls.attachMedia(videoRef.current);
  }, [call]);

  return (
    <>
      <ViewerHeader />
      // this is the video that HLS.js would use to playback the stream.
      <video autoPlay playsInline ref={videoRef} onPlay={() => setIsPlaying(true)} />

      {!isPlaying && <LoadingIndicator />}
      <ViewerControls />
    </>
  );
};
```

Having done this, we now have a fully functional live-streaming experience for the viewers based on HLS.

:::tip

For simplicity, a few things are omitted from the code snippet above. For the full-blown and working example,
check this Livestreaming sample app:

- [Livestream App source code](https://github.com/GetStream/stream-video-js/tree/main/sample-apps/react/livestream-app)
- [Livestream App live preview](https://video-react-livestream-app.vercel.app/)

:::
